{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fe6a36",
   "metadata": {},
   "source": [
    "# 1. Integrácia dát (3b)\n",
    "\n",
    "- Integrácia datasetu - vhodne zakomponujte zvolené informácie o počasí.\n",
    "- Sampling – vytvorenie vzorky z datasetu (veľkosti napr. 10%) pri zachovaní rozloženia cieľového atribútu.\n",
    "- Rozdelenie datasetu na trénovaciu a testovaciu množinu (napr. v pomere 60/40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a110c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T09:08:46.732451100Z",
     "start_time": "2025-04-08T09:08:46.657110800Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fddabb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T09:08:48.071188100Z",
     "start_time": "2025-04-08T09:08:48.052127300Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_files(spark: SparkSession, file_paths: list[str], infer_schema:bool=True, header:bool=True, on:str=\"id\", how:str=\"inner\") -> DataFrame:\n",
    "    if len(file_paths) == 0:\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", IntegerType(), True),\n",
    "            StructField(\"name\", StringType(), True)\n",
    "        ])\n",
    "        df = spark.createDataFrame([],schema)\n",
    "    else:\n",
    "        df = spark.read.csv(file_paths[0], header=header, inferSchema=infer_schema)\n",
    "        for idx in range(1,len(file_paths)):\n",
    "            file = spark.read.csv(file_paths[idx], header=header, inferSchema=infer_schema)\n",
    "            file = file.withColumnRenamed(\"Vehicle_Reference\", f\"Vehicle_Reference_{idx}\")\n",
    "            df = df.join(file, on=on, how=how)\n",
    "            print(df.columns)\n",
    "            print(f'df{idx}--------------')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1fa58f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T09:08:51.176457Z",
     "start_time": "2025-04-08T09:08:51.131402900Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_by_percent(df: DataFrame, label: str, percent: float) -> DataFrame:\n",
    "    fractions = df.select(label).distinct().rdd.map(lambda r: (r[0], percent)).collectAsMap()\n",
    "    return df.stat.sampleBy(label, fractions, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c55de5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T09:14:22.755737900Z",
     "start_time": "2025-04-08T09:14:16.745603500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accident_Index', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude', 'Latitude', 'Police_Force', 'Accident_Severity', 'Number_of_Vehicles', 'Number_of_Casualties', 'Date', 'Day_of_Week', 'Time', 'Local_Authority_(District)', 'Local_Authority_(Highway)', '1st_Road_Class', '1st_Road_Number', 'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities', 'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Carriageway_Hazards', 'Urban_or_Rural_Area', 'Did_Police_Officer_Attend_Scene_of_Accident', 'LSOA_of_Accident_Location', 'Vehicle_Reference_1', 'Casualty_Reference', 'Casualty_Class', 'Sex_of_Casualty', 'Age_of_Casualty', 'Age_Band_of_Casualty', 'Casualty_Severity', 'Pedestrian_Location', 'Pedestrian_Movement', 'Car_Passenger', 'Bus_or_Coach_Passenger', 'Pedestrian_Road_Maintenance_Worker', 'Casualty_Type', 'Casualty_Home_Area_Type']\n",
      "df1--------------\n",
      "['Accident_Index', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude', 'Latitude', 'Police_Force', 'Accident_Severity', 'Number_of_Vehicles', 'Number_of_Casualties', 'Date', 'Day_of_Week', 'Time', 'Local_Authority_(District)', 'Local_Authority_(Highway)', '1st_Road_Class', '1st_Road_Number', 'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities', 'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Carriageway_Hazards', 'Urban_or_Rural_Area', 'Did_Police_Officer_Attend_Scene_of_Accident', 'LSOA_of_Accident_Location', 'Vehicle_Reference_1', 'Casualty_Reference', 'Casualty_Class', 'Sex_of_Casualty', 'Age_of_Casualty', 'Age_Band_of_Casualty', 'Casualty_Severity', 'Pedestrian_Location', 'Pedestrian_Movement', 'Car_Passenger', 'Bus_or_Coach_Passenger', 'Pedestrian_Road_Maintenance_Worker', 'Casualty_Type', 'Casualty_Home_Area_Type', 'Vehicle_Reference_2', 'Vehicle_Type', 'Towing_and_Articulation', 'Vehicle_Manoeuvre', 'Vehicle_Location-Restricted_Lane', 'Junction_Location', 'Skidding_and_Overturning', 'Hit_Object_in_Carriageway', 'Vehicle_Leaving_Carriageway', 'Hit_Object_off_Carriageway', '1st_Point_of_Impact', 'Was_Vehicle_Left_Hand_Drive?', 'Journey_Purpose_of_Driver', 'Sex_of_Driver', 'Age_of_Driver', 'Age_Band_of_Driver', 'Engine_Capacity_(CC)', 'Propulsion_Code', 'Age_of_Vehicle', 'Driver_IMD_Decile', 'Driver_Home_Area_Type']\n",
      "df2--------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CarAccidents\").getOrCreate()\n",
    "\n",
    "file_list = [\n",
    "    \"../datalab/TSVD/dataset/CarAccidents/Accidents.csv\",\n",
    "    \"../datalab/TSVD/dataset/CarAccidents/Casualties.csv\",\n",
    "    \"../datalab/TSVD/dataset/CarAccidents/Vehicles.csv\"\n",
    "]\n",
    "\n",
    "df = read_csv_files(spark, file_list, on=\"Accident_Index\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d71d6dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T09:26:15.877677100Z",
     "start_time": "2025-04-08T09:26:11.160575300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Accident_Index: string, Location_Easting_OSGR: int, Location_Northing_OSGR: int, Longitude: double, Latitude: double, Police_Force: int, Accident_Severity: int, Number_of_Vehicles: int, Number_of_Casualties: int, Date: string, Day_of_Week: int, Time: timestamp, Local_Authority_(District): int, Local_Authority_(Highway): string, 1st_Road_Class: int, 1st_Road_Number: int, Road_Type: int, Speed_limit: int, Junction_Detail: int, Junction_Control: int, 2nd_Road_Class: int, 2nd_Road_Number: int, Pedestrian_Crossing-Human_Control: int, Pedestrian_Crossing-Physical_Facilities: int, Light_Conditions: int, Weather_Conditions: int, Road_Surface_Conditions: int, Special_Conditions_at_Site: int, Carriageway_Hazards: int, Urban_or_Rural_Area: int, Did_Police_Officer_Attend_Scene_of_Accident: int, LSOA_of_Accident_Location: string, Vehicle_Reference_1: int, Casualty_Reference: int, Casualty_Class: int, Sex_of_Casualty: int, Age_of_Casualty: int, Age_Band_of_Casualty: int, Casualty_Severity: int, Pedestrian_Location: int, Pedestrian_Movement: int, Car_Passenger: int, Bus_or_Coach_Passenger: int, Pedestrian_Road_Maintenance_Worker: int, Casualty_Type: int, Casualty_Home_Area_Type: int, Vehicle_Reference_2: int, Vehicle_Type: int, Towing_and_Articulation: int, Vehicle_Manoeuvre: int, Vehicle_Location-Restricted_Lane: int, Junction_Location: int, Skidding_and_Overturning: int, Hit_Object_in_Carriageway: int, Vehicle_Leaving_Carriageway: int, Hit_Object_off_Carriageway: int, 1st_Point_of_Impact: int, Was_Vehicle_Left_Hand_Drive?: int, Journey_Purpose_of_Driver: int, Sex_of_Driver: int, Age_of_Driver: int, Age_Band_of_Driver: int, Engine_Capacity_(CC): int, Propulsion_Code: int, Age_of_Vehicle: int, Driver_IMD_Decile: int, Driver_Home_Area_Type: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sampled_data = sample_by_percent(df,\"Accident_Severity\",0.1)\n",
    "print(f\"Percent for sample data: {(sampled_data.count()/df.count())*100}\")\n",
    "# sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73a1c7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accident_Index', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude', 'Latitude', 'Police_Force', 'Accident_Severity', 'Number_of_Vehicles', 'Number_of_Casualties', 'Date', 'Day_of_Week', 'Time', 'Local_Authority_(District)', 'Local_Authority_(Highway)', '1st_Road_Class', '1st_Road_Number', 'Road_Type', 'Speed_limit', 'Junction_Detail', 'Junction_Control', '2nd_Road_Class', '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control', 'Pedestrian_Crossing-Physical_Facilities', 'Light_Conditions', 'Weather_Conditions', 'Road_Surface_Conditions', 'Special_Conditions_at_Site', 'Carriageway_Hazards', 'Urban_or_Rural_Area', 'Did_Police_Officer_Attend_Scene_of_Accident', 'LSOA_of_Accident_Location', 'Vehicle_Reference_1', 'Casualty_Reference', 'Casualty_Class', 'Sex_of_Casualty', 'Age_of_Casualty', 'Age_Band_of_Casualty', 'Casualty_Severity', 'Pedestrian_Location', 'Pedestrian_Movement', 'Car_Passenger', 'Bus_or_Coach_Passenger', 'Pedestrian_Road_Maintenance_Worker', 'Casualty_Type', 'Casualty_Home_Area_Type', 'Vehicle_Reference_2', 'Vehicle_Type', 'Towing_and_Articulation', 'Vehicle_Manoeuvre', 'Vehicle_Location-Restricted_Lane', 'Junction_Location', 'Skidding_and_Overturning', 'Hit_Object_in_Carriageway', 'Vehicle_Leaving_Carriageway', 'Hit_Object_off_Carriageway', '1st_Point_of_Impact', 'Was_Vehicle_Left_Hand_Drive?', 'Journey_Purpose_of_Driver', 'Sex_of_Driver', 'Age_of_Driver', 'Age_Band_of_Driver', 'Engine_Capacity_(CC)', 'Propulsion_Code', 'Age_of_Vehicle', 'Driver_IMD_Decile', 'Driver_Home_Area_Type']\n",
      "Accident_Index: 343502 dropped\n",
      "Location_Easting_OSGR: 86424 dropped\n",
      "Location_Northing_OSGR: 99499 dropped\n",
      "Longitude: 288831 dropped\n",
      "Latitude: 311336 dropped\n",
      "Police_Force: 53\n",
      "Accident_Severity: 3\n",
      "Number_of_Vehicles: 26\n",
      "Number_of_Casualties: 50\n",
      "Date: 3478\n",
      "Day_of_Week: 7\n",
      "Time: 1397\n",
      "Local_Authority_(District): 423\n",
      "Local_Authority_(Highway): 204\n",
      "1st_Road_Class: 6\n",
      "1st_Road_Number: 5300\n",
      "Road_Type: 6\n",
      "Speed_limit: 7\n",
      "Junction_Detail: 10\n",
      "Junction_Control: 6\n",
      "2nd_Road_Class: 7\n",
      "2nd_Road_Number: 5656\n",
      "Pedestrian_Crossing-Human_Control: 4\n",
      "Pedestrian_Crossing-Physical_Facilities: 7\n",
      "Light_Conditions: 5\n",
      "Weather_Conditions: 10\n",
      "Road_Surface_Conditions: 6\n",
      "Special_Conditions_at_Site: 9\n",
      "Carriageway_Hazards: 7\n",
      "Urban_or_Rural_Area: 3\n",
      "Did_Police_Officer_Attend_Scene_of_Accident: 4\n",
      "LSOA_of_Accident_Location: 34315 dropped\n",
      "Vehicle_Reference_1: 66\n",
      "Casualty_Reference: 76\n",
      "Casualty_Class: 3\n",
      "Sex_of_Casualty: 3\n",
      "Age_of_Casualty: 103\n",
      "Age_Band_of_Casualty: 12\n",
      "Casualty_Severity: 3\n",
      "Pedestrian_Location: 12\n",
      "Pedestrian_Movement: 11\n",
      "Car_Passenger: 4\n",
      "Bus_or_Coach_Passenger: 6\n",
      "Pedestrian_Road_Maintenance_Worker: 4\n",
      "Casualty_Type: 21\n",
      "Casualty_Home_Area_Type: 4\n",
      "Vehicle_Reference_2: 67\n",
      "Vehicle_Type: 21\n",
      "Towing_and_Articulation: 7\n",
      "Vehicle_Manoeuvre: 19\n",
      "Vehicle_Location-Restricted_Lane: 11\n",
      "Junction_Location: 10\n",
      "Skidding_and_Overturning: 7\n",
      "Hit_Object_in_Carriageway: 13\n",
      "Vehicle_Leaving_Carriageway: 10\n",
      "Hit_Object_off_Carriageway: 13\n",
      "1st_Point_of_Impact: 6\n",
      "Was_Vehicle_Left_Hand_Drive?: 3\n",
      "Journey_Purpose_of_Driver: 8\n",
      "Sex_of_Driver: 4\n",
      "Age_of_Driver: 98\n",
      "Age_Band_of_Driver: 12\n",
      "Engine_Capacity_(CC): 1578\n",
      "Propulsion_Code: 11\n",
      "Age_of_Vehicle: 74\n",
      "Driver_IMD_Decile: 11\n",
      "Driver_Home_Area_Type: 4\n"
     ]
    }
   ],
   "source": [
    "# train_data, test_data = df.randomSplit([0.6, 0.4], seed=42)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(sampled_data.columns)\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "agg_result = sampled_data.agg(*[\n",
    "    F.approx_count_distinct(c).alias(c)\n",
    "    for c in sampled_data.columns\n",
    "]).first().asDict()\n",
    "\n",
    "# print(\"Уникальные значения по столбцам:\")\n",
    "for col, count in agg_result.items():\n",
    "    if count > 9999:\n",
    "        sampled_data = sampled_data.drop(col)\n",
    "        print(f\"{col}: {count} dropped\")\n",
    "    else:\n",
    "        print(f\"{col}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63c01718-0044-4e51-b93d-8c245727e3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local_Authority_(Highway): StringType()\n"
     ]
    }
   ],
   "source": [
    "# df_cleaned = df.dropna()\n",
    "\n",
    "sd = sampled_data.drop('Date')\n",
    "# sd = sd.drop('Accident_Index')\n",
    "# sd = sd.drop('Local_Authority_(Highway)')\n",
    "# sd = sd.drop('LSOA_of_Accident_Location')\n",
    "\n",
    "for col_name in sd.columns:\n",
    "    col_type = sd.schema[col_name].dataType\n",
    "    if isinstance(col_type, StringType):\n",
    "        print(f'{col_name}: {col_type}')\n",
    "# sd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be152884",
   "metadata": {},
   "source": [
    "# 2. Predspracovanie (7b)\n",
    "\n",
    "- Transformácia nominálnych atribútov na numerické\n",
    "- Transformácia numerických atribútov na nominálne\n",
    "- Vypočítanie pomerového kritéria – informačného zisku voči cieľovému atribútu (klasifikačná úloha), pre nominálne atribúty\n",
    "- Vypočítanie štatistík pre numerické atribúty\n",
    "- Vytvorenie histogramov pre nominálne atribúty\n",
    "- Spracovanie chýbajúcich hodnôt (napr. ich nahradenie priemermi, atď.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6b79dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, count, mean, when, isnull, isnan, lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, QuantileDiscretizer, Imputer\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# def read_csv_files(spark: SparkSession, file_paths: list[str], infer_schema:bool=True, header:bool=True, on:str=\"id\", how:str=\"inner\") -> DataFrame:\n",
    "#     if len(file_paths) == 0:\n",
    "#         schema = StructType([\n",
    "#             StructField(\"id\", IntegerType(), True),\n",
    "#             StructField(\"name\", StringType(), True)\n",
    "#         ])\n",
    "#         df = spark.createDataFrame([],schema)\n",
    "#     else:\n",
    "#         df = spark.read.csv(file_paths[0], header=header, inferSchema=infer_schema)\n",
    "#         for idx in range(1,len(file_paths)):\n",
    "#             file = spark.read.csv(file_paths[idx], header=header, inferSchema=infer_schema)\n",
    "#             file = file.withColumnRenamed(\"Vehicle_Reference\", f\"Vehicle_Reference_{idx}\")\n",
    "#             df = df.join(file, on=on, how=how)\n",
    "#     return df\n",
    "\n",
    "# def sample_by_percent(df: DataFrame, label: str, percent: float) -> DataFrame:\n",
    "#     fractions = df.select(label).distinct().rdd.map(lambda r: (r[0], percent)).collectAsMap()\n",
    "#     return df.stat.sampleBy(label, fractions, seed=42)\n",
    "\n",
    "def preprocess_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Main preprocessing function implementing all requested tasks\"\"\"\n",
    "    \n",
    "    # 1. Identify column types\n",
    "    categorical_cols = [field.name for field in df.schema.fields \n",
    "                      if isinstance(field.dataType, StringType)]\n",
    "    numerical_cols = [field.name for field in df.schema.fields \n",
    "                     if isinstance(field.dataType, (IntegerType, FloatType, DoubleType))]\n",
    "    numerical_cols.remove('Accident_Severity')\n",
    "    \n",
    "    # 2. Transform nominal attributes to numerical\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\") \n",
    "               for col in categorical_cols]\n",
    "    \n",
    "    pipeline_stages = indexers\n",
    "    \n",
    "    # 3. Transform numerical attributes to nominal (binning)\n",
    "    for num_col in numerical_cols:\n",
    "        if num_col != \"Accident_Index\":  # Skip ID columns\n",
    "            discretizer = QuantileDiscretizer(numBuckets=5, \n",
    "                                            inputCol=num_col, \n",
    "                                            outputCol=num_col+\"_category\",\n",
    "                                            handleInvalid=\"keep\",\n",
    "                                            relativeError=0.01)\n",
    "            pipeline_stages.append(discretizer)\n",
    "    \n",
    "    # 4. Information Gain calculation will be done separately\n",
    "    \n",
    "    # Cache the input DataFrame if used multiple times\n",
    "    df.cache()  # or df.persist() with specific storage level\n",
    "    pipeline = Pipeline(stages=pipeline_stages)\n",
    "    pipeline_model = pipeline.fit(df)  # Fit once\n",
    "    preprocessed_df = pipeline_model.transform(df)  # Transform\n",
    "    \n",
    "    # Unpersist when done\n",
    "    df.unpersist()\n",
    "    \n",
    "    return preprocessed_df\n",
    "\n",
    "def calculate_information_gain(df: DataFrame, target_col: str = \"Accident_Severity\"):\n",
    "    \"\"\"Calculate information gain for nominal attributes against target\"\"\"\n",
    "    from pyspark.ml.feature import ChiSqSelector\n",
    "    \n",
    "    # Get all indexed categorical columns\n",
    "    categorical_index_cols = [col for col in df.columns if col.endswith(\"_index\")]\n",
    "    binned_cols = [col for col in df.columns if col.endswith('_category')]\n",
    "    categorical_index_cols = categorical_index_cols + binned_cols\n",
    "    # print(categorical_index_cols)\n",
    "    \n",
    "    # Create feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=categorical_index_cols,\n",
    "        outputCol=\"features\")\n",
    "    \n",
    "    feature_df = assembler.transform(df)\n",
    "    # print('feature_df cols')\n",
    "    # feature_df.printSchema()\n",
    "    \n",
    "    # Calculate chi-squared stats (related to information gain)\n",
    "    selector = ChiSqSelector(numTopFeatures=10, \n",
    "                           featuresCol=\"features\", \n",
    "                           outputCol=\"selectedFeatures\", \n",
    "                           labelCol=target_col)\n",
    "    \n",
    "    # Get and display the chi-squared test results\n",
    "    model = selector.fit(feature_df)\n",
    "    result = model.transform(feature_df)\n",
    "\n",
    "    # TODO CAN BE USEFUL\n",
    "    best_cols_indices = model.selectedFeatures\n",
    "    from pyspark.ml.stat import ChiSquareTest\n",
    "    \n",
    "    # List of all categorical columns (original + binned)\n",
    "    all_categorical_cols = [col for idx, col in enumerate(categorical_index_cols) if idx in best_cols_indices]  \n",
    "    \n",
    "    # Compute information gain (via chi-squared) for each column\n",
    "    ig_results = []\n",
    "    for col in all_categorical_cols:\n",
    "         # Create a single-feature vector for each column\n",
    "        assembler = VectorAssembler(inputCols=[col], outputCol=\"feature_vec\")\n",
    "        df_vec = assembler.transform(df)\n",
    "        \n",
    "        # Run ChiSquareTest on the vectorized feature\n",
    "        chi2 = ChiSquareTest.test(df_vec, \"feature_vec\", target_col).head()\n",
    "        \n",
    "        ig_results.append({\n",
    "            \"Feature\": str(col),\n",
    "            \"Chi2_Statistic\": float(chi2.statistics.toArray()[0]),\n",
    "            \"pValue\": float(chi2.pValues[0]),\n",
    "            \"DegreesOfFreedom\": chi2.degreesOfFreedom,\n",
    "            \"Information_Gain\": float(chi2.statistics.toArray()[0]) / (2 * df_vec.count())\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and sort by IG\n",
    "    ig_df = spark.createDataFrame(ig_results).orderBy(\"information_gain\", ascending=False)\n",
    "    ig_df.show(truncate=False)\n",
    "    \n",
    "    return model, result\n",
    "\n",
    "def calculate_numerical_stats(df: DataFrame):\n",
    "    \"\"\"Calculate statistics for numerical attributes\"\"\"\n",
    "    numerical_cols = [col for col in df.columns \n",
    "                     if any(col.endswith(suffix) for suffix in [\"\", \"_imputed\"]) \n",
    "                     and not col.endswith((\"_index\", \"_category\"))]\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats = df.select(numerical_cols).describe().toPandas().set_index('summary')\n",
    "    print(\"Basic statistics:\")\n",
    "    print(stats)\n",
    "    \n",
    "    # Correlation matrix\n",
    "    numerical_df = df.select(numerical_cols)\n",
    "    corr_matrix = Statistics.corr(numerical_df.rdd.map(lambda row: [float(x) for x in row]), method=\"pearson\")\n",
    "    corr_df = pd.DataFrame(corr_matrix, columns=numerical_cols, index=numerical_cols)\n",
    "    print(\"\\nCorrelation matrix:\")\n",
    "    print(corr_df)\n",
    "    \n",
    "    return stats, corr_df\n",
    "\n",
    "def create_histograms(df: DataFrame, nominal_cols: list):\n",
    "    \"\"\"Create histograms for nominal attributes\"\"\"\n",
    "    for col_name in nominal_cols:\n",
    "        if col_name in df.columns:\n",
    "            # Get value counts\n",
    "            value_counts = df.groupBy(col_name).count().orderBy(\"count\", ascending=False).toPandas()\n",
    "            \n",
    "            # Plot histogram\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(value_counts[col_name].astype(str), value_counts[\"count\"])\n",
    "            plt.title(f\"Histogram of {col_name}\")\n",
    "            plt.xlabel(col_name)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ecd4acc1-5513-42f7-a7d6-6a822406c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local_Authority_(Highway): StringType()\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column Local_Authority_(Highway) is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m target_col = \u001b[33m'\u001b[39m\u001b[33mAccident_Severity\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mshow_ig_by_col\u001b[49m\u001b[43m(\u001b[49m\u001b[43msd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mig_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36mshow_ig_by_col\u001b[39m\u001b[34m(df, cols, target_col)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols:\n\u001b[32m    146\u001b[39m      \u001b[38;5;66;03m# Create a single-feature vector for each column\u001b[39;00m\n\u001b[32m    147\u001b[39m     assembler = VectorAssembler(inputCols=[col], outputCol=\u001b[33m\"\u001b[39m\u001b[33mfeature_vec\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     df_vec = \u001b[43massembler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# Run ChiSquareTest on the vectorized feature\u001b[39;00m\n\u001b[32m    151\u001b[39m     chi2 = ChiSquareTest.test(df_vec, \u001b[33m\"\u001b[39m\u001b[33mfeature_vec\u001b[39m\u001b[33m\"\u001b[39m, target_col).head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/ml/base.py:262\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/ml/wrapper.py:398\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: Data type string of column Local_Authority_(Highway) is not supported."
     ]
    }
   ],
   "source": [
    "# testing ground for showing ig\n",
    "ig_cols = []\n",
    "for col in sd.columns:\n",
    "    col_type = preprocessed_df.schema[col].dataType\n",
    "    if isinstance(col_type, StringType):\n",
    "        ig_cols.append(col)\n",
    "        print(f'{col}: {col_type}')\n",
    "\n",
    "target_col = 'Accident_Severity'\n",
    "show_ig_by_col(sd, ig_cols, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e14e849d-1c4f-435e-8087-2cab5f318187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing done\n"
     ]
    }
   ],
   "source": [
    "# 3. Preprocess data\n",
    "%matplotlib inline\n",
    "# print(sd.columns)\n",
    "preprocessed_df = preprocess_data(sd)\n",
    "print('preprocessing done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8293066f-4bf4-40bb-8cf1-8160fdd66704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [\"age_index\", \"Age_Index\", \"Accident_Index\", \"income_index\"]\n",
    "# [index_col for index_col in cols if index_col.endswith(\"_index\")]\n",
    "# preprocessed_df.printSchema()\n",
    "for col in preprocessed_df.columns:\n",
    "    if 'category' in col:\n",
    "        col_type = preprocessed_df.schema[col].dataType\n",
    "        # print(col)\n",
    "        # print(col_type)\n",
    "# [index_col for index_col in cols if index_col.endswith(\"_index\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2f50733-17fc-445a-99de-bc6c2a19a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_df.columns\n",
    "\n",
    "# Split data\n",
    "# train_data, test_data = preprocessed_df.randomSplit([0.6, 0.4], seed=42)\n",
    "# print('splitting done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "045fd377-2f13-43ef-87c2-5f578b5f2f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Gain Analysis:\n",
      "+------------------+----------------+-----------------------------------+---------------------+------+\n",
      "|Chi2_Statistic    |DegreesOfFreedom|Feature                            |Information_Gain     |pValue|\n",
      "+------------------+----------------+-----------------------------------+---------------------+------+\n",
      "|10634.363599288286|[6]             |Speed_limit_category               |0.012387346560442786 |0.0   |\n",
      "|10373.552110504139|[4]             |Number_of_Casualties_category      |0.01208354255107729  |0.0   |\n",
      "|9717.734724935726 |[410]           |Local_Authority_(Highway)_index    |0.011319619335592807 |0.0   |\n",
      "|4496.3925122187475|[4]             |Number_of_Vehicles_category        |0.005237583970173943 |0.0   |\n",
      "|1707.5663716821252|[4]             |Road_Type_category                 |0.0019890439351161523|0.0   |\n",
      "|1607.2833538624996|[6]             |1st_Road_Class_category            |0.0018722301282286485|0.0   |\n",
      "|1177.2397018903887|[8]             |Police_Force_category              |0.001371297495696364 |0.0   |\n",
      "|1120.0798936346994|[8]             |Local_Authority_(District)_category|0.0013047153868958835|0.0   |\n",
      "|750.5146481410446 |[8]             |Day_of_Week_category               |8.742305036320274E-4 |0.0   |\n",
      "|331.2761157358911 |[6]             |1st_Road_Number_category           |3.8588412127383686E-4|0.0   |\n",
      "+------------------+----------------+-----------------------------------+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Calculate information gain\n",
    "print(\"\\nInformation Gain Analysis:\")\n",
    "ig_model, results = calculate_information_gain(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67054664-d26d-4b2a-bf60-783c69db695b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(selectedFeatures=DenseVector([109.0, 0.0, 1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0])),\n",
       " Row(selectedFeatures=DenseVector([109.0, 0.0, 1.0, 1.0, 3.0, 0.0, 1.0, 1.0, 2.0, 1.0])),\n",
       " Row(selectedFeatures=DenseVector([109.0, 0.0, 0.0, 1.0, 2.0, 0.0, 2.0, 3.0, 2.0, 1.0])),\n",
       " Row(selectedFeatures=DenseVector([109.0, 0.0, 2.0, 2.0, 2.0, 0.0, 1.0, 4.0, 2.0, 1.0])),\n",
       " Row(selectedFeatures=DenseVector([109.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 4.0, 2.0, 1.0]))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.select('selectedFeatures').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a56cd9-5e85-4ad8-99b8-5df247087711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Calculate numerical statistics\n",
    "# last error here\n",
    "print(\"\\nNumerical Statistics:\")\n",
    "stats, corr_matrix = calculate_numerical_stats(preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c20845b-1054-4488-b1e8-efddb459f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Create histograms\n",
    "print(\"\\nCreating histograms...\")\n",
    "categorical_cols = [col for col in preprocessed_df.columns \n",
    "                   if isinstance(preprocessed_df.schema[col].dataType, StringType)]\n",
    "create_histograms(preprocessed_df, categorical_cols[:5])  # First 5 to avoid too many plots\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae4f76",
   "metadata": {},
   "source": [
    "# 3. Modelovanie - Vytvorenie popisných modelov (3b):\n",
    "\n",
    "- Vytvorte k-means clustering model\n",
    "- Pomocou vytvoreného modelu detekujte anomálie\n",
    "# 4. Modelovanie - Vytvorenie klasifikačných modelov typu (aspoň jeden model každého typu)(4b):\n",
    "\n",
    "- Decision tree model\n",
    "- Linear SVM\n",
    "- Naive Bayes model\n",
    "- Ensembles of decision trees (Random Forests, Gradient-boosted trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b7af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2754253",
   "metadata": {},
   "source": [
    "# 5. Vyhodnotenie (3b)\n",
    "\n",
    "- Natrénovanie klasifikačného modelu na trénovacej množine a jeho evaluáciu na testovacej množine.\n",
    "- Klasifikačný model vyhodnocujte použitím kontigenčnej tabuľky a vypočítaním metrík presnosti, návratnosti, F1 a MCC (Matthews Correlation Coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561e288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
